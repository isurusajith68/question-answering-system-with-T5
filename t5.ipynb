{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 841/841 [00:00<00:00, 9647.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 841/841 [00:00<00:00, 1815.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 672\n",
      "Eval dataset size: 169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load dataset from a JSON file\n",
    "dataset = load_dataset(\"json\", data_files=\"improved_mcq_dataset.json\")\n",
    "\n",
    "# Preprocessing function to format the input and target\n",
    "def preprocess_data(example):\n",
    "    input_text = (\n",
    "        f\"context: {example['context']} \"\n",
    "        f\"question: {example['question']} \"\n",
    "        f\"options: {', '.join(example['options'])}\"\n",
    "    )\n",
    "    target_text = example['answer']\n",
    "    return {\"input_text\": input_text, \"target_text\": target_text}\n",
    "    \n",
    "# Apply the preprocessing function to the dataset\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"context\", \"question\", \"options\", \"answer\"])\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_data(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], max_length=512, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], max_length=128, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the data\n",
    "dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Split the dataset into training and evaluation sets using train_test_split\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Access the training and evaluation datasets\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from transformers[torch]) (1.1.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from requests->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isuru\\anaconda3\\envs\\new\\lib\\site-packages (from jinja2->torch->transformers[torch]) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'options', 'answer'],\n",
      "        num_rows: 841\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the JSON file (improved_mcq_dataset.json)\n",
    "dataset = load_dataset(\"json\", data_files=\"improved_mcq_dataset.json\")\n",
    "\n",
    "# Print the dataset to verify it's loaded correctly\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 672/672 [00:00<00:00, 2429.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:00<00:00, 1931.48 examples/s]\n",
      "c:\\Users\\Isuru\\anaconda3\\envs\\new\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load dataset from JSON file\n",
    "dataset = load_dataset(\"json\", data_files=\"improved_mcq_dataset.json\")\n",
    "\n",
    "# If no train/test split exists, create one\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)  # 80% for training, 20% for validation\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Ensure 'context' and 'question' exist in the dataset\n",
    "    inputs = [f\"context: {context} question: {question}\" for context, question in zip(examples['context'], examples['question'])]\n",
    "    \n",
    "    # Output is the correct answer (as one of the options)\n",
    "    targets = examples['answer']\n",
    "    \n",
    "    # Tokenize inputs and targets using T5 tokenizer\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Add labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for saving model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=8,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,  # Batch size per device during evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir=\"./logs\",  # Directory to store logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    save_steps=10_000,  # Save model every 10k steps\n",
    "    save_total_limit=2,  # Only keep the 2 most recent checkpoints\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to be trained\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"test\"],  # Evaluation dataset (validation)\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isuru\\anaconda3\\envs\\new\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Generating train split: 613 examples [00:00, 24517.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [00:00<00:00, 1919.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 490\n",
      "Test dataset size: 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 490/490 [00:00<00:00, 1817.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:00<00:00, 1845.97 examples/s]\n",
      "c:\\Users\\Isuru\\anaconda3\\envs\\new\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  5%|â–Œ         | 10/186 [01:18<24:49,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.5458, 'grad_norm': 90.13682556152344, 'learning_rate': 1.89247311827957e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 20/186 [02:58<27:20,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.37, 'grad_norm': 80.87954711914062, 'learning_rate': 1.78494623655914e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 30/186 [04:35<25:18,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6048, 'grad_norm': 72.12053680419922, 'learning_rate': 1.6774193548387098e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 40/186 [06:12<23:51,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2055, 'grad_norm': 45.716583251953125, 'learning_rate': 1.5698924731182796e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 50/186 [07:50<21:31,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0929, 'grad_norm': 31.496084213256836, 'learning_rate': 1.4623655913978497e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 60/186 [09:10<16:36,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4973, 'grad_norm': 12.022537231445312, 'learning_rate': 1.3548387096774194e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/186 [10:02<12:56,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17746637761592865, 'eval_runtime': 41.5421, 'eval_samples_per_second': 2.961, 'eval_steps_per_second': 0.385, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 70/186 [11:13<18:27,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1678, 'grad_norm': 5.6366448402404785, 'learning_rate': 1.2473118279569894e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 80/186 [12:35<14:41,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.997, 'grad_norm': 3.985795021057129, 'learning_rate': 1.1397849462365593e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/186 [14:03<13:25,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8925, 'grad_norm': 4.458528995513916, 'learning_rate': 1.0322580645161291e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/186 [15:24<11:38,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6992, 'grad_norm': 4.402638912200928, 'learning_rate': 9.24731182795699e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 110/186 [16:49<11:24,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5862, 'grad_norm': 4.469958305358887, 'learning_rate': 8.172043010752689e-06, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/186 [18:22<10:12,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4839, 'grad_norm': 34.22441482543945, 'learning_rate': 7.096774193548388e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 124/186 [19:33<07:26,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18215292692184448, 'eval_runtime': 41.2974, 'eval_samples_per_second': 2.978, 'eval_steps_per_second': 0.387, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/186 [20:29<10:19, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4767, 'grad_norm': 3.7115869522094727, 'learning_rate': 6.021505376344087e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 140/186 [22:12<07:54, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4045, 'grad_norm': 2.528980016708374, 'learning_rate': 4.946236559139785e-06, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 150/186 [23:48<05:47,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3835, 'grad_norm': 1.982576847076416, 'learning_rate': 3.870967741935484e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 160/186 [25:12<03:35,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3607, 'grad_norm': 2.90108323097229, 'learning_rate': 2.7956989247311827e-06, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 170/186 [26:56<02:40, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3507, 'grad_norm': 1.7168809175491333, 'learning_rate': 1.720430107526882e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 180/186 [28:26<00:56,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3461, 'grad_norm': 2.3854501247406006, 'learning_rate': 6.451612903225807e-07, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [29:52<00:00,  9.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18632686138153076, 'eval_runtime': 37.4234, 'eval_samples_per_second': 3.287, 'eval_steps_per_second': 0.428, 'epoch': 3.0}\n",
      "{'train_runtime': 1792.0578, 'train_samples_per_second': 0.82, 'train_steps_per_second': 0.104, 'train_loss': 2.4022324495418097, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model\\\\tokenizer_config.json',\n",
       " './fine_tuned_model\\\\special_tokens_map.json',\n",
       " './fine_tuned_model\\\\spiece.model',\n",
       " './fine_tuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"improved_mcq_dataset.json\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"Context: {context} Question: {question} Options: {', '.join(options)}\"\n",
    "        for context, question, options in zip(examples['context'], examples['question'], examples['options'])\n",
    "    ]\n",
    "    \n",
    "    targets = examples['answer']\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data = dataset[\"train\"]  \n",
    "data_dict = {\n",
    "    \"context\": data[\"context\"],\n",
    "    \"question\": data[\"question\"],\n",
    "    \"options\": data[\"options\"],\n",
    "    \"answer\": data[\"answer\"]\n",
    "}\n",
    "\n",
    "data_list = [{'context': c, 'question': q, 'options': o, 'answer': a} \n",
    "             for c, q, o, a in zip(data_dict[\"context\"], data_dict[\"question\"], data_dict[\"options\"], data_dict[\"answer\"])]\n",
    "\n",
    "train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'context': [item['context'] for item in train_data],\n",
    "    'question': [item['question'] for item in train_data],\n",
    "    'options': [item['options'] for item in train_data],\n",
    "    'answer': [item['answer'] for item in train_data]\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'context': [item['context'] for item in test_data],\n",
    "    'question': [item['question'] for item in test_data],\n",
    "    'options': [item['options'] for item in test_data],\n",
    "    'answer': [item['answer'] for item in test_data]\n",
    "})\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for saving model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=8,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,  # Batch size per device during evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir=\"./logs\",  # Directory to store logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    save_steps=10_000,  # Save model every 10k steps\n",
    "    save_total_limit=2,  # Only keep the 2 most recent checkpoints\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MCQ: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "def generate_mcq(context):\n",
    "    input_text = f\"Context: {context} Generate a question and four answer options. Ensure one option is correct.\"\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    output = model.generate(input_ids=inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_answer\n",
    "\n",
    "\n",
    "# Example test\n",
    "context = \"Cloud computing allows users to access and store data over the internet. It provides on-demand availability of computing resources such as servers, storage, and databases.\"\n",
    "generated_mcq = generate_mcq(context)\n",
    "\n",
    "print(f\"Generated MCQ: {generated_mcq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
